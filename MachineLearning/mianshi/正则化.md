
## 正则化的做法以及背后的思想

正则化是一种回归方法，将参数估计向零的方向进行约束、调整或缩小，也就是说，正则化可以在学习过程中降低模型复杂度和不稳定度，从而避免过拟合的危险。

### L1正则化

L1正则化也称Lasso正则化，应用L1范数，将参数稀疏化，可以达到特征选择的目的。

从最大后验概率的角度来分析，L1默认参数$\omega$服从拉普拉斯分布，加入了拉普拉斯分布的先验知识。

### L2正则化

L2正则化也称Ridge正则化，岭回归，权值衰减，应用L2范数，将参数向0的方向约束，但并不使其变成0。

从最大后验概率的角度分析，L2默认参数$\omega$服从高斯分布，加入了高斯分布的先验知识。

### L1和L2的比较

采用L1正则化时，平方误差曲线和正则化等值线通常相交在坐标轴处，但采用L2正则化的时候，该交点通常出现在象限中。

这就意味着在L1中，将正则化系数$\lambda$设置的足够大的时候，会迫使一些系数的估计值完全等于0，也就是说参数w中只有对应参数非0的特征才会被用来组成模型，达到了特征选择的作用，L1正则化是一种嵌入式特征选择方法，特征选择和模型训练同时完成，模型具有可解释性;

而L2将不重要的特征的系数趋近于0但永不等于0,使得模型的解偏向于范数较小的$\omega$，通过限制$\omega$的大小实现了对模型空间的限制，在一定程度上避免了overfitting，这意味着最终的模型会包含所有的特征，从计算量上并没有任何改观。但L2正则化可以有效的提高求解的稳定性和快速，因为L2的引入会使得ill-condition的情况得以改善，缓解在condition number不好的条件下矩阵求逆很难的问题。



## Batch Normalization

BN的方法是用来处理Internal Covarite Shift问题的，由于机器学习的一个重要基础是训练空间和测试空间满足独立同分布原则IID，但由于经过浅层的变换，使得原本IID的数据发生变化，层层叠加之下，高层的数据分布变化非常剧烈，使得网络的收敛非常慢。

BN的基本思想很直观：因为深度神经网络在做非线性变换钱的激活输入值随着网络的加深或者在训练过程中其分布逐渐发生便宜或者变动，训练速度慢的原因大致是
BN的由来是白化操作，研究发现输入图像的白化操作可以使得网络很快收敛，所以Google的研究者尝试在每层之间做白化操作，并强行将数据变成均值为0，方差为1，并使得满足IID原则。

这个方法给每层的输出都做了一次归一化，（网络上相当于加了一个线性变化层），使得下一层的输入结金高斯分布，相当于下一层的w训练时避免了其输入以偏概全，因而泛化效果好。

## Covariance Shift