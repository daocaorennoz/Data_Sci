# 评估方法

## 分类算法的评估方法

- False Negative 假阴性

    样本标签为负，判定为负

- False Postive 假阳性
    样本标签为负，判定为正

- True Negative 真阴性

    样本标签为正，判定为负

- True Postive 真阳性

    样本标签为正，判定为正

由此衍生出precision和recall

- Precision 精确率

    计算公式为
    $$\frac{TP}{TP + FP}$$
    意思为在所有模型预测为正的所有样本中，真实预测正确的比例，也叫查准率。

- Recall 召回率

    计算公式为
    $$\frac{TP}{TP + FN}$$
    针对样本中所有的正例，模型预测出来的部分，也叫查全率。
    或者叫True Postive rate

- Accuracy 准确率

    计算公式为
    $$\frac{TP + TN}{TP + TN + FN + FP}$$
    意思是所有判断正确的数据占全体数据的比例。

- ROC曲线 Receiver Operating Characteristic受试者工作特征


    - TPR:true positive rate, 等于$\frac{TP}{TP + FN}$,又叫precision rate 
    - FPR:false positive rate,等于$\frac{FP}{TN + FP}$
    - TNR:true negative rate,等于 $\frac{TN}{TN + FP}$
    - FNR:false negative rate,等于$\frac{FN}{TP + FN}$，又叫miss rate 没有分类正确的正例在所有正例的比例

    是以FPR（假阳率）为横坐标，TPR（真阳率）为纵坐标绘制成的曲线，ROC曲线下面的面积成为AUC值（Area Under Curve），
    从AUC判断分类器（预测模型）优劣的标准：

    - AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
    - 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
    - AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
    - AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

    一句话来说，AUC值越大的分类器，正确率越高。

- PR曲线 Precision-Recall

在非平衡的数据集上，即使AUC值很大的模型可能效果也不好，因为正类的样本实在太小所以假阳率肯定很小，接近于0，而Precision综合考虑了TP和FP的值，所以在极度不平衡的情况下，PR会比ROC曲线更加实用。





