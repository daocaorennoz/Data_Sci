# 集成学习

## 集成学习的分类

- Bagging 方法
    简述：是一种独立投票方法。Bagging是Boostrap Aggregating的简称，意思就是再取样，然后在每个样本上训练出来的模型取平均。Boostrap的操作对训练数据做了一定程度的扰动，造成分类器的多样性，由于基分类器优化的目标是一致的，因此variance会随着集成的过程而降低，所以对于每个基分类器而言，目标变成如何降低偏差bias，所以会采用深度很深甚至不加剪枝的决策树。

- Boosting 方法
    简述：是一种提升方法。Boosting在迭代的过程中不断优化Loss function，拟合原数据，减少的是bias，对于每个弱学习器而言，目标在于如何选择variance更小的分类器，更简单的分类器，所以可以选择深度很浅的决策树。

## Bagging 

Bagging被称为是自举汇聚法，思想是在原始数据集上，通过又放回的抽样，重新选择S个新的数据集来分别训练S个分类器的集成技术。

Bagging方法训练出的模型在预测新样本分类的时候，会使用多数投票，求均值的方式来统计最终分类结果。

Bagging方法的弱学习器可以是基本的算法模型：线性回归，岭回归，Lasso回归，Logistic回归，Softmax、ID3、C4.5、CART、SVM、KNN等。

- 随机森林
    
    随机森林是非常典型的应用了Bagging思想的集成模型。
    算法流程：
     - 从原始样本集中用Bootstrap采样-有放回的选出m个样本
     - 从所有属性中随机选出k个属性，选择最佳分个属性作为节点创建决策树
     - 重复上面个两个步骤s次，建立s个决策树。
     - 这s个决策树形成随机森林，通过投票表决结果，决定数据属于哪一类。

     随机森林包含两个随机的层面：样本随机，选择特征随机

     没有必要对随机森林进行交叉验证，或用独立的测试集来获取误差的无偏估计。因为构建每个树的时候有一部分样本没有参加模型的训练，可以称之为oob(out of bag)样本，所以通过袋外错误率可以评估随机森林的表现。

     随机森林的变体：

     - Extra Tree

     原理和随机森林基本一样，区别如下：

        - RF会随机采样作为子决策树的训练集，而Extra Tree每个子决策树采用原始数据集训练。
        - RF在选择划分特征点的时候会和传统决策树一样，会基于信息增益、信息增益率、基尼系数、均方差等原则选择最优的特征值，Extra Tree则是随机选择一个特征值来划分决策树。
        
     由于随机选择特征点进行划分，所以得到的决策树规模大于RF，方差会减少，泛化能力比RF强。

     - Totally Random Tree Embedding（TRTE）

     是一种非监督的数据转化方式，，将低维数据映射到高维，让高维数据更利于分类回归。

    经过TRTE转化后的编码可以用于无监督的分类操作，将相似的特征码聚类到一起，最后完成分类的操作。

    - Isolation Forest（IForest）

    是一种异常点检测算法，使用类似RF的方式来检测异常点。

    区别在于：
        
        - 随机采样的过程中，只需要少量的数据
        - 决策树的构建过程中，IForest随机选择一个特征划分，并对划分特征随机选择一个划分阈值。
        - Iforest的划分深度比较小，Max-depth比较小。

    RF的优缺点：
        
    优点：

        - 训练可以并行化，并对大规模样本的训练具有速度的优势
        - 由于进行随机选择决策树划分特征列表，在样本维度比较高的时候，仍然具有比较高的训练性能。
        - 可以给出各个特征值的重要性列表（可以用来进行特征筛选）
        - RF实现简单
        - 对于部分特征的丢失不敏感（树模型的通用优点）

    缺点：

        - 噪音过大的特征上，容易过拟合
        - 取值比较多的划分特征对RF的决策会产生更大的影响，从而影响模型的效果。

## Boosting

通过迭代训练一系列的弱分类器，训练过程中注重分错的样本。其代表算法是AdaBoost, GBDT，XGBoost。

### AdaBoost算法

Adaptive Boost 是一种迭代算法，每一轮迭代之后生成一个新的学习器，然后对样本进行预测。预测对的权重减小，预测错的权重增加。权重越高的在下一轮的迭代中占的比重就越大，即越难区分的样本在样本中越重要。

整个迭代错误率够小或迭代次数到一定次数停止。

AdaBoost算法将基分类器的线性组合作为强分类器，同时给分类误差率较小的基分类器以大的权值，给误差率较大的基分类器以小的权值。
基分类器的线性组合为
$$
f(x)=\sum_{m=1}^{M} \alpha_{m} G_{m}(x)
$$

最后的强分类器公式为：

$$
G(x)=sign(f(x))=sign\left[\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right]
$$

而此时损失函数为：

$$
loss=\frac{1}{n} \sum_{i=1}^{n} I\left(G\left(x_{i}\right) \neq y_{i}\right)
$$

所以需要学习的参数就是基分类器及其对应的权重。

分类误差为：

$$
\varepsilon_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{n} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)
$$

基分类器权重设定公式：
$$
\alpha_{m}=\frac{1}{2} * \log _{2}\left(\frac{1-\varepsilon_{m}}{\varepsilon}\right)
$$

样本权重更新公式（从均匀分布更新）：
$$
D_{\mathrm{m}+1}=\left(w_{m+1,1}, w_{m+1,2}, \ldots w_{m+1, i} \ldots, w_{m+1, n}\right) \quad w_{m+1, i}=\frac{W_{m, i}}{Z_{m}} e^{-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)}
$$

最终得到的强分类器：

$$
G(x)=sign(f(x))=sign\left[\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right]
$$

### GBDT梯度提升迭代决策树

Gradient Boosting Decision Tree

也是Boosting算法的一种，但与AdaBoost有所区别：

- AdaBoost算法利用前一轮的弱学习器的误差来更新样本权重值，然后一轮轮迭代。更改的是样本输入X
- GBDT同样是迭代，但要求弱学习器必须是CART模型，并且GBDT再模型训练的时候是要求模型预测的样本损失尽可能小。利用残差作为下一轮优化的目标来进行迭代。更改的是拟合目标输出Y

