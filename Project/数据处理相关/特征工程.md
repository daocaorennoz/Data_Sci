# 特征工程

## 简单的数字技巧

数字数据是已经数学模型易于消化的格式了，但这并不意味着不再需要特征工程，好的特征不仅代表着数据的显著方面，而且符合模型的假设。

数值数据的第一个检查为大小是否重要，还是我们只需要知道其正负面，或者只需要知道一个非常粗粒度的范围。

然后考虑数值的范围，最大值最小值是多少，跨越几个数量级，输入特性平滑的模型对输出数据的量级敏感，其输入的规模直接决定了输出的规模，例如简单的线性函数，以及k-means，knn，RBF内核以及一切用到欧几里得距离的模型和模型组件。

逻辑函数对输入不敏感，通常是跃迁函数。

考虑数值特征的分布也很重要，总结了承担特定价值的可能性，输入特征的分布对某些模型比其他模型更重要。线性回归模型的训练过程假定预测误差分布得像高斯。这通常是好的，除非预测目标在几个数量级上扩散。在这种情况下，高斯误差假设可能不再成立。解决这一问题的一种方法是转变产出目标，以驯服规模的增长。（严格地说，这将是目标工程，而不是特征工程。）对数变换，这是一种功率变换，将变量的分布接近高斯。

多个特征可以组合成更多的复杂特征，使得输入特征更加雄辩，而模型更加简单。更容易训练和泛化，做出更好的预测。

### 二值化

在大数据的时代，计数可以快速积累而不受约束。当数据以很高的速度和体积增长的时候，很可能产生一些极值，检查其规模，确定是否保持他们作为原始特征，还是转为二进制变量，以示存在或者将其放入粗粒度。

例如在音乐系统的推荐中，不能简单的衡量一首播放次数为20的歌曲的受欢迎程度是播放次数为1的歌曲20倍，如果将计数二元化和修剪所有大于1的计数为1，换句话说，如果用户至少听过一首歌，那么我们将其视为用户喜欢歌曲，这样模型不需要花费周期来预测原始数据之间的微小差异，二进制目标是用户偏好的简单而稳健的度量。

### 量化或者装箱

相当于二值化的提升版，我们知道很多模型的输入一旦跨越数量级就是有问题的。

一种解决方案是通过量化计数来包含标量。换句话说，我们将计数分组到容器中，并且去掉实际的计数值。量化将连续数映射成离散数。我们可以把离散化的数字看作是代表强度度量的容器的有序的序列。

解决方案分为固定宽度和自适应宽度。

- 固定宽度

    选择合适的宽度来对整个数据集进行划分重组。但可能宽度的选择会导致出现空箱的出现。

- 自适应宽度
    
    - 分位数装箱

        按照数据分布的自适应定位来解决。

### 对数转换

对数变换是处理具有重尾分布的正数的有力工具。（重尾分布在尾部范围内的概率比高斯分布的概率大）。它将分布在高端的长尾压缩成较短的尾部，并将低端扩展成较长的头部。


- 功率变换

    对数变换的一种推广。

- Box-Cox变换

    $\tilde{x}=\left\{\begin{array}{ll}{\frac{x^{2}-1}{\lambda}} & {\text { if } \lambda \neq 0} \\ {\ln (x)} & {\text { if } \lambda=0}\end{array}\right.$

    平方根变换和对数变换的简单推广称为Box-Cox变换。只有当数值为正的时候Box-Cox才会工作。

简单的对数变换和最优的 Box-Cox 变换都使正尾部接近正态分布。最优的 Box-Cox 变换比对数变换更缩小尾部

### 特征缩放和归一化

某些特征的值有界的，如纬度或经度。其他数值特征 (如数量) 可能会在无界的情况下增加。那些关于输入是平滑函数的模型, 如线性回归、逻辑回归或任何涉及矩阵的东西, 都受输入的数值范围影响。另一方面, 基于树的模型不太在意这个。如果你的模型对输入特征的数值范围敏感, 则特征缩放可能会有所帮助。顾名思义, 特征缩放会更改特征值的数值范围。有时人们也称它为特征规范化。功能缩放通常分别针对单个特征进行。有几种常见的缩放操作, 每个类型都产生不同的特征值分布。

- Min-max缩放

$$\tilde{x}=\frac{x-\min (x)}{\max (x)-\min (x)}$$

Min-max缩放压缩（或拉伸）所有特征值到[0, 1 ]的范围内。

- 标准化

$$\tilde{x}=\frac{x- mean(x)}{var(x)}$$

减去特征 (所有数据点) 的平均值并除以方差。因此, 它也可以称为方差缩放。缩放后的特征的平均值为0, 方差为1。如果原始特征具有高斯分布, 则缩放特征为标准高斯。

### 不要中心化稀疏数据

最小最大缩放和标准化都从原始特征值中减去一个数量。如果移动量不是零, 则这两种转换可以将稀疏特征（大部分值为零）的向量转换为一个稠密的向量。

- L2正则化

    $$\tilde{x}=\frac{x}{\|x\|_{2}}$$

    L2正则化后，特征具有L2范数，也可以成为L2缩放，度量向量在坐标空间中的长度。

### 交互特征

简单的成对交互特征是两个特征的积。类似逻辑与。它以成对条件表示结果。
这一点对基于决策树的模型没有影响，但发交互特征对广义线性模型通常很有帮助。

一个简单的线性模型使用单个输入特征线性组合x1，x2，... xn来预测结果y
$$
y=w_{I} X_{I}+w_{2} X_{2}+\ldots+w_{n} X_{n}
$$
一个简单的扩展线性模型的方法是包含输入特征对的组合，如下所示：

$$
y=w_{I} X_{j}+w_{2} X_{2}+\cdots+w_{n} X_{h}+w_{1,1}X_{1}X_{1}+w_{1,2}X_{1}X_{2}+w_{1,3}X_{1}X_{3}+\cdots
$$

这使我们能够捕获特征之间的相互影响，因此它们被称为交互特征。构造交互特征非常简单，但它们使用起来很昂贵。使用成对交互特征的线性模型的训练和得分时间将从$O(n)$到$O(n^2)$，其中n是单身特征的数量。

围绕高阶交互特征的计算成本有几种方法。可以在所有交互特征之上执行特征选择，选择前几个。或者，可以更仔细地制作更少数量的复杂特征。两种策略都有其优点和缺点。特征选择采用计算手段来选择问题的最佳特征.

### 特征选择

特征选择技术会删除非有用的特征，以降低最终模型的复杂性。最终目标是快速计算的简约模型，预测准确性降低很小或不会降低。为了得到这样的模型，一些特征选择技术需要训练多个候选模型。换句话说，特征选择并不是减少训练时间，实际上有些技巧增加了整体训练时间，但是减少了模型评分时间。
简单分成三类：

- Filtering（过滤）：预处理可以删除那些不太可能对模型有用的特征。可以计算每个特征与响应变量之间的相关或相互信息，并筛除相关信息或相互信息低于阈值的特征。但是他们没有考虑到正在使用的模型。因此他们可能无法为模型选择正确的特征。最好先保守地进行预过滤，以免在进行模型训练步骤之前无意中消除有用的特征。

- Wrapper methods（包装方法）：这些技术是昂贵的，但它们允许您尝试特征子集，这意味着你不会意外删除自身无法提供信息但在组合使用时非常有用的特征。包装方法将模型视为提供特征子集质量分数的黑盒子。是一个独立的方法迭代地改进子集。

- Embedded methods（嵌入式方法）：嵌入式方法执行特征选择作为模型训练过程的一部分。最经典的嵌入式方法是L1正则化。嵌入式方法将特征选择作为模型训练过程的一部分。它们不如包装方法那么强大，但也远不如包装方法那么昂贵。与过滤相比，嵌入式方法会选择特定于模型的特征。从这个意义上讲，嵌入式方法在计算费用和结果质量之间取得平衡。

## 文本数据

### 词袋模型 bag of words

词袋模型不考虑单词之间的联系，单纯的统计每个单词出现的次数，然后转成平面的一维向量。

文档被转为向量，向量只是n个数字的集合，向量中包含词汇表中每个单词可能出现的数目。里面不包含任何的文本结构，不代表任何词层次结构的概念，没有概念的知识。所以两个句子如果组成的单词表一样的化，那么表示向量就是一致的。对于简单的文本分类任务和信息检索任务，简单的字数统计通常比较适用。

### Bag of N-gram

Bag of N-gram 是BOW的自然扩展，当N=1的时候，Bag of N-gram退化成BOW，n-gram是n个有序列的记号（token） ，N-gram 保留了文本的更多原始序列结构，故 bag-of-n-gram可以提供更多信息。但是，这是有代价的。理论上，用 k 个独特的词，可能有 k 个独立的 2-gram（也称为 bi-gram）。在实践中，并不是那么多，因为不是每个单词后都可以跟一个单词。尽管如此，通常有更多不同的 n-gram（n > 1）比单词更多。这意味着词袋会更大并且有稀疏的特征空间。这就意味着存储和训练的成本更高，n越大，信息越丰富，成本越高。


### 过滤和清洗特征

- 去除停用词

    分类和检索通常不需要对文本有深入的理解，代词、冠词和介词大部分时间并没有显示出其价值。很多流行的python处理NLP的包都自带停用词表，可以直接调用去除，


