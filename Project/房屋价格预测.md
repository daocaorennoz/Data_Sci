
# 房屋价格预测

## 数据集探索

训练集：目标SalePrice维在内，共计有81维特征，共计1460条记录。

测试集：共计有80维特征，1459条记录

训练集中SalePrice的分布情况：
    
    峰度：6.536282
    偏度：1.882876

目标维房屋价格的数据分布的偏度时1.882876，峰度是6.536282；可以大致看出变量正偏差较大，长尾拖在右侧，数据分布也比较陡峭。

Tips:

- 峰度：Kurtosis是描述某变量所有取值分布形态抖缓程度的统计量
    是与正态分布相比较的。

    - Kurtosis  = 0 代表与正态分布抖缓程度相同
    - Kurtosis > 0 代表比正态分布的高峰更加陡峭-- 尖顶峰
    - Kurtosis < 0 代表比正态分布的高峰来的更平坦 - 平顶峰
    
- 偏度：Skewness是描述变量取值分布对成型的统计量

    - Skewness = 0 分布形态与正态分布偏度相同
    - Skewness > 0 正偏差数值较大，为正偏或者右偏，长尾巴托在右边
    - Skewness < 0 负偏差数值较大，为负偏或者左偏，长尾巴托在左边

数据缺失情况：
有19维特征存在缺失值，且有6维特征缺失部分占比高于15%。

特征中共有38维类别型变量，37维数值型变量。

维度之间的可视化：

对于类别型变量，使用箱图来查看其与SalePrice的关系；
对于数值型变量，使用distplot来查看其与SalePrice的关系。

但这种可视化的方法适用于特征维数比较少的情况，所以很难在实际情况中使用，只能通过对原始特征的了解之后才能找到经验主义上的主要特征变量，然后观察是否特征是否重要。


## 数据预处理

###缺失值填充

将情况分为两部分处理，分别为数值型和类别型：
处理方案为数值型用众数来代替，类别型用None来填充。

### 特征选择

将缺失情况超过15%的特征维删除。

在特征选择的时候，首先对部分数值型特征进行可视化，查看是否与目标维非常相关。
但由于数据维度过多，该方法耗时耗力，并不采用。

于是利用spearman系数绘制热力图，查看数值型变量之间的关系。

- 通过corr()方法查看各特征与SalePrice之间的相关系数，筛选出>0.5的特征。
- 然后查看筛选出来的特征之间的相关性，是否具有多重共线性。通过

## 模型调参



## Kaggle上的思路

### 思路一

- 首先利用distplot查看SalePrice的分布情况，调用峰度和偏度的计算公式计算。
- 然后对数值型变量，使用散点图进行绘制

    ```python
    #scatter plot grlivarea/saleprice
    var = 'GrLivArea'
    data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
    data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));
    ```

- 对类别型变量，使用箱图进行绘制

    ```python
    #box plot overallqual/saleprice
    var = 'OverallQual'
    data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
    f, ax = plt.subplots(figsize=(8, 6))
    fig = sns.boxplot(x=var, y="SalePrice", data=data)
    fig.axis(ymin=0, ymax=800000);
    ```

- 对选出的几个变量进行成对分析

    ```python
    #scatterplot
    sns.set()
    cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']
    sns.pairplot(df_train[cols], size = 2.5)
    plt.show()
    ```

- 进行缺失值的分析

    ```python
    #missing data
    total = df_train.isnull().sum().sort_values(ascending=False)
    percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)
    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
    missing_data.head(20)
    ```
    做出的处理是将缺失值全部删掉，只有一条的删除那一条记录。很粗暴。

- 单变量分析 Univariate analysis

    首先标准化数据，然后得出离群点

    ```python
    #standardizing data
    saleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]) 
    # np.newaxis在这里的作用是按列索引之后仍然得到列的结构
    low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]
    high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]
    print('outer range (low) of the distribution:')
    print(low_range)
    print('\nouter range (high) of the distribution:')
    print(high_range)
    ```

- 删除离群点

- 变换SalePrice

    由上文可知，SalePrice不是标准的正态分布，偏度正偏，较为陡峭。
    经过取对数之后，发现SalePrice的分布和正太拟合的比较好了。

- 创造新的特征

    - 通过TotalBsmtSF来创造新的特征HasBsmt
    ```python
    #create column for new variable (one is enough because it's a binary categorical feature)
    #if area>0 it gets 1, for area==0 it gets 0
    df_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)
    df_train['HasBsmt'] = 0 
    df_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1
    ```
    其实是为了将有0值的HasBsmt的特征也正太化。
    这种方法被称为利用二值法来正态化特征。

参考：
    https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python

### 思路二

Stacking Regressions

- 对SalePrice进行标准化，使用$log(1+x)$进行转换

    ```python
    #We use the numpy fuction log1p which  applies log(1+x) to all elements of the column
    train["SalePrice"] = np.log1p(train["SalePrice"])
    ```
- 特征工程

    将训练集除去SalePrice之后和测试集连在一起，一起做特征工程

    - 缺失值处理
        填充缺失值，不删除任何特征，类别型用None，数值型有的用中位数，有的用0
    
    - 对一些类别特征值进行labelEncoder

    - 创造组合特征
    ```python
    # Adding total sqfootage feature 
    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']
    ```

    - 使用Box Cox对倾斜度大的特征进行转换
    ```python
    skewness = skewness[abs(skewness) > 0.75]
    print("There are {} skewed numerical features to Box Cox transform".format(skewness.shape[0]))

    from scipy.special import boxcox1p
    skewed_features = skewness.index
    lam = 0.15
    for feat in skewed_features:
        #all_data[feat] += 1
        all_data[feat] = boxcox1p(all_data[feat], lam)
        
    #all_data[skewed_features] = np.log1p(all_data[skewed_features])
    ```

    - 对类别型变量进行单热编码

- 建模
    
    - import

    ```python
    from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC
    from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor
    from sklearn.kernel_ridge import KernelRidge
    from sklearn.pipeline import make_pipeline
    from sklearn.preprocessing import RobustScaler
    from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
    from sklearn.model_selection import KFold, cross_val_score, train_test_split
    from sklearn.metrics import mean_squared_error
    import xgboost as xgb
    import lightgbm as lgb
    ```

    - k折交叉验证k-cross-validation
    ```python
    #Validation function
    n_folds = 5

    def rmsle_cv(model):
        kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
        rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring="neg_mean_squared_error", cv = kf))
        return(rmse)
    ```

    - LASSO Regression
    该模型对离群点非常敏感，所以需要提前进行缩放。
    ```python
    lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))
    ```
    其中make_pipeline是用来进行串行化的。

    - Elastic Net Regression

    ```python
    ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))
    ```

    - Kernel Ridge Regression
    ```python
    KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)
    ```

    - Gradient Boosting Regression
    ```python
    GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,
                                   max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10, 
                                   loss='huber', random_state =5)
    ```

    - XGBoost
    ```python
    model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, 
                             learning_rate=0.05, max_depth=3, 
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state =7, nthread = -1)
    ```

    - LightGB
    ```python
    model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)
    ```

    - 评估每个简单模型的得分

    - Stacking模型

        - 简单平均模型

        ```python
        class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):
            def __init__(self, models):
                self.models = models
                
            # we define clones of the original models to fit the data in
            def fit(self, X, y):
                self.models_ = [clone(x) for x in self.models]
                
                # Train cloned base models
                for model in self.models_:
                    model.fit(X, y)

                return self
            
            #Now we do the predictions for cloned models and average them
            def predict(self, X):
                predictions = np.column_stack([
                    model.predict(X) for model in self.models_
                ])
                return np.mean(predictions, axis=1)
        ```

        ```python
        averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))
        score = rmsle_cv(averaged_models)
        print(" Averaged base models score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))
        ```


        - 训练元模型

        将训练集划分成训练的和留存的，使用基础模型再训练的上进行训练，然后再留存的部分上得到回归值，然后将预测的到的回归值作为元特征，使用模型将元特征与目标回归值之间建立联系，称之为元模型。

        ```python
        class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):
            def __init__(self, base_models, meta_model, n_folds=5):
                self.base_models = base_models
                self.meta_model = meta_model
                self.n_folds = n_folds
        
            # We again fit the data on clones of the original models
            def fit(self, X, y):
                self.base_models_ = [list() for x in self.base_models]
                self.meta_model_ = clone(self.meta_model)
                kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)
                
                # Train cloned base models then create out-of-fold predictions
                # that are needed to train the cloned meta-model
                out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
                for i, model in enumerate(self.base_models):
                    for train_index, holdout_index in kfold.split(X, y):
                        instance = clone(model)
                        self.base_models_[i].append(instance)
                        instance.fit(X[train_index], y[train_index])
                        y_pred = instance.predict(X[holdout_index])
                        out_of_fold_predictions[holdout_index, i] = y_pred
                        
                # Now train the cloned  meta-model using the out-of-fold predictions as new feature
                self.meta_model_.fit(out_of_fold_predictions, y)
                return self
        
            #Do the predictions of all base models on the test data and use the averaged predictions as 
            #meta-features for the final prediction which is done by the meta-model
            def predict(self, X):
                meta_features = np.column_stack([
                    np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)
                    for base_models in self.base_models_ ])
                return self.meta_model_.predict(meta_features)
        ```

        ```python
        stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),
                                                 meta_model = lasso)
        score = rmsle_cv(stacked_averaged_models)
        print("Stacking Averaged models score: {:.4f} ({:.4f})".format(score.mean(), score.std()))
        ```
参考：
https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard
