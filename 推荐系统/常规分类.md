# 推荐系统

## 基于内容的推荐系统

利用项目的内在品质或者固有属性来进行推荐，比如说音乐的流派，类型和电影的风格和类别，不需要建立User-Item矩阵，时间里在项目的内容信息上做出的推荐，不需要依据用户对项目的评价意见，更多地需要用机器学习的方法从关于内容的特征描述的事例中得到用户的兴趣资料。基于项目的相似度来通过最近邻获取与目标项目最相似的项目列表，然后把用户没有行为记录的高评分的项目推荐给特定用户。

## 基于协同过滤的推荐

通过集体智慧的力量进行工作，过滤掉用户不感兴趣的项目。协同过滤是基于这样的假设：为特定用户找到真正感兴趣的内容的方法是首先找到于此用户具有相似兴趣的其他用户，然后将他们感兴趣的内容推荐给此用户。

一般采用最近邻的思想，利用用户的历史喜好计算用户之间的距离，然后利用目标用户的最近邻居用户对商品评价的加权评价值来预测目标用户对特定用户的喜好程度，系统根据喜好程度来对目标用户进行对剑，通常需要构建UI矩阵，协同过滤又可以根据是否运用机器学习思想进一步划分为基于内存的协同过滤以及基于模型的协同过滤。

### 基于内存的协同过滤 Memory-based CF

主要通过启发式的方法来进行推荐，主要步骤一个是相似性函数的选择，如何选择合适的相似性函数来更好的度量两个项目或者用户的相似性，另一个是推荐策略的选择，最简单的推荐策略就是推荐那些大多数人产生过行为而目标用户未产生过行为的项目。

根据用户维度和项目维度可以分为Item-Based CF和User-Based CF。

- Item-Based CF

    [1] 构建UI矩阵
    [2] 根据UI矩阵来计算列的相似度，即项目之间的相似度
    [3] 根据特定项目最相似的k个项目构成推荐列表
    [4] 推荐给特定用户列表中还没有发生过行为的项目

- User-Based CF

    [1] 构建UI矩阵
    [2] 根据UI矩阵来计算行之间的相似度，即用户之间的相似度
    [3] 选择特定用户最相似的k个用户
    [4] 推荐给特定用户列表中还没发生过行为而在相似用户列表中产生过行为的高频项目

### 基于模型的协同过滤 Model-Based CF

#### 基于矩阵分解的推荐

对于M（M个item）行，N（N个user）列的稀疏矩阵，用户对项目的评分是不充分的，任务是通过分析已有的数据来对未知数据进行预测，矩阵填充的任务，该任务可以通过矩阵分解来实现。

- SVD矩阵分解 奇异值分解

    假设矩阵M是一个m\*n的矩阵，则一定存在一个分解$M = U\Sigma V^T$，其中U是m\*m的正交矩阵，V是n\*n的正交矩阵，$\Sigma$ 是m*n的对角阵，所有元素非负，且依次减小，可以适用最大的k个值和对应大小的U、V矩阵来近似描述原始的评分矩阵。
    
    但传统的SVD要求矩阵是稠密的，所以需要对原始矩阵进行简单的填充。

- FunkSVD

    在原始的SVD上进行改进，将原始评分矩阵M（m\*n）分解为两个矩阵P（m\*k）和Q
    (k\*n)，同时考察原始评分矩阵中有评分的项分解是否准确。

    对于原始矩阵中有评分的位置$M_{UI}$而言，分解后矩阵中对应的值为

    $$
    \mathrm{M}_{\mathrm{uI}}^{\prime}=\sum_{k=1}^{K} P_{U, k} Q_{k, I}
    $$

    对于整个矩阵而言总损失为：

    $$
    S S E=E^{2}=\sum_{U, 1}\left(M_{U, I}-M_{U, I}^{\prime}\right)^{2}
    $$

    目标为最小化损失SSE，就能一最小的扰动完成对原始评分矩阵的分解。之后计算M‘的方式完成原始评分矩阵的填充。

    这种方法被称为隐语义模型（Latent factor model），通过隐含特征将user与item特征联系起来。

    可以发现用户U对物品I的最终评分为由各个隐含特征维度下U对I感兴趣程度的和。

    现在的问题就变成了如何求出使得SSE最小的矩阵P和Q，可以用随机梯度下降法求解。

- FunkSVD的改进

    [1] 正则化

    $$
    \mathrm{SSE}=\sum_{U, I}\left(M_{U, I}-M_{U, I}^{\prime}\right)^{2}+\lambda \sum_{U} P_{U}^{2}+\lambda \sum_{I} Q_{I}^{2}
    $$

    其中$\lambda$为正则化系数，求解过程依然可以适用随机梯度下降法。

    [2] 偏置 bias

    上文中U对I的评分全部由U和I之间的联系带来的，而实际上，有很多性质是用户和物品独有的，某个用户非常严苛，给分很低，某个物品非常精美，给分很高，仅仅与物品自身相关。

    所以不妨让整个评分矩阵的平均分为$\sigma$，用户和物品的偏置分别为$b_U$和$b_I$,评分计算公式为：

    $$
    \mathrm{M}_{\mathrm{UI}}^{\prime}=\sigma+\mathrm{b}_{\mathrm{U}}+\mathrm{b}_{\mathrm{l}}+\sum_{k=1}^{K} P_{U, k} Q_{k, I}
    $$

    误差计算公式也变成：

    $$
    \mathrm{SSE}=\sum_{U, 1}\left(M_{U, I}-M_{U, l}^{\prime}\right)^{2}+\lambda \sum_{U} P_{U}^{2}+\lambda \sum_{I} Q_{I}^{2}+\lambda \sum_{U} b_{U}^{2}+\lambda \sum_{I} b_{I}^{2}
    $$

    [3] 隐式反馈SVD++

    加上用户没有转化为评分的隐式行为，这类隐式行为非常庞大，利用了隐式行为之后的计算方式变为：

    $$
    \mathrm{M}_{\mathrm{UI}}^{\prime}=\sigma+\mathrm{b}_{\mathrm{U}}+\mathrm{b}_{\mathrm{I}}+\sum_{k=1}^{K}\left(P_{U, k}+\frac{1}{\sqrt{|N(U)|}} \sum_{j \in N(U)} y_{j k}\right) Q_{k, I}
    $$

    相应的损失项也要加上正则化，

    $$
    \mathrm{SSE}=\sum_{U, 1}\left(M_{U, I}-M_{U, l}^{\prime}\right)^{2}+\lambda \sum_{U} P_{U}^{2}+\lambda \sum_{I} Q_{I}^{2}+\lambda \sum_{U} b_{U}^{2}+\lambda \sum_{I} b_{I}^{2} + \lambda \sum_{j\in N(u)}y_j^2
    $$

    [4] SVD++的对偶算法

    上面的SVD++是基于用户的角度来考虑问题的，所以可以同样基于物品的角度来考虑问题：

    $$
    \mathrm{M}_{\mathrm{UI}}^{\prime}=\sigma+\mathrm{b}_{\mathrm{U}}+\mathrm{b}_{\mathrm{I}}+\sum_{k=1}^{K}P_{U, k}\left(\frac{1}{\sqrt{|N(U)|}} \sum_{j \in N(U)} y_{j k} + Q_{k, I}\right)
    $$

    实际运用中，可以将原始的SVD++得到的结果与对偶算法得到的结果进行融合，使得预测更加准确。

    [5] 因子分解机FM

    [6] 与DNN的结合

    Youtube的推荐模型：首先通过nlp的计数与训练出所有物品的向量I表示，对于每一条用户对物品的点击，将用户的历史典籍、历史搜索、地理位置信息等信息经过各自的embedding操作，拼接起来作为输入，经过MLP训练之后得到用户的向量表示U，最终通过softmax函数来校验U\*I的结果是否准确。

    DNN可以为模型带来非线性分部分，提高你和能力，可以方便加入各式各样的特征，提高模型准确度。

    矩阵分解的优缺点：
    - 优点： 
        1.可以将高维矩阵映射成两个低维矩阵的乘积，很好解决数据稀疏问题，
        2.具体实现和求解都很简洁，预测精度也很好
        3.模型的可扩展性非常优秀，基本思想广泛运用各种场景之中。
    
    - 缺点：
        1.可解释性差，隐空间中的维度很难与显示的概念对应起来，
        2.训练速度慢，不过可以通过离线训练来弥补缺点
        3. 实际推荐场景中只关心topn结果的准确性，此时考察全局的均方差显然是不准确的。
